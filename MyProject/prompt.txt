I am trying to write a proof in lean. I want to prove that a quadratic has a minimum value in the reals when a > 0
I have chosen to use the form of the quadratic equation as f(x) = a(x - k)2 + bx +c
ie I want to prove
∃ x₀, ∀ x, f(x₀) <= f(x) when a>0
Let us test the value x₀ = k
is the statement still true?
∀ x, f(k) <= f(x) when a>0
a(k - k)^2 + c <= a(x-k)^2 + c
c <= a(x-k)^2 + c
0 <= a(x-k)^2
∀ x, k in ℝ, (x-k)^2 >= 0
also we know a > 0 from our initial assumption
so when a is multiplied by (x-k)^2 the result will always be nonnegative. therefore the statement 0 <= a(x-k)^2 is true ∀ x, k in ℝ and a > 0
Have I missed any steps?
Can you help me translate that to lean?
This is my attempt at the lean program which needs to be finished
import Mathlib.Analysis.Calculus.Deriv.Basic import Mathlib.Data.Real.Basic import Mathlib.Tactic
open Real
def quadratic (a k c x : ℝ) : ℝ := a * (x - k) ^ 2 + c
theorem quadratic_has_minimum (a k c : ℝ) (ha : 0 < a) : ∃ x₀, ∀ x, a * (x - k) ^ 2 + c ≥ a * (x₀ - k)^ 2 + c := by sorry

-----------------------------------


I am designing an image background subtraction algorithm 
Images are a 2 dimensional grid of real numbers. I represent them with functions of two variables x and y

I, B, S: ℕ x ℕ -> ℝ

The given image: I(x,y) 
The known Background image: B(x,y)
The desired signal to be computed: S(x,y)

I want to perform this signal separation:

S(x,y) = I(x,y) - pB(x,y)

where p is some real number I have to find a way to chose optimally


I have decided to chose p such that it minimises the resultant 'edginess' of the resultant signal S(x,y)

ie choose p to minimise R(p):

R(p) = sum(||grad(S(x,y))||^2)

I next substitute in the definition of S(x,y)

R(p) = sum(||grad(I(x,y) - pB(x,y))||^2)

expanding:

R = sum(grad(I(x,y)) .grad(I(x,y))) - 2p. sum(grad(B(x,y)) .grad(I(x,y))) + (p^2)sum(grad(B(x,y)) .grad(B(x,y)))

Which is now my quadratic in p
which I want to show has a minimum value

It should also be noted here that we need sum(grad(B(x,y)) .grad(B(x,y))) > 0
Effectively this means that the user can not use a background image which is blank as this will cause division by zero.

Can you help me translate these ideas to lean?

----------------------------------------------


hmm I already have a proof of the vertex for case

theorem vertex_quadratic_has_minimum (a h k : ℝ) (ha : 0 < a) :
    ∃ x₀, ∀ x, quadratic_vertex a h k x ≥ quadratic_vertex a h k x₀ := by
  use h
  intro x
  unfold quadratic_vertex
  have h1 : 0 ≤ (x - h) ^ 2 := sq_nonneg (x - h)
  have h2 : 0 ≤ a * (x - h) ^ 2 := mul_nonneg (le_of_lt ha) h1
  calc
    a * (x - h) ^ 2 + k ≥ 0 + k := add_le_add_right h2 k
    _ = a * (h - h) ^ 2 + k := by simp_all only
    [
      mul_nonneg_iff_of_pos_left,
      zero_add,
      sub_self,
      ne_eq,
      OfNat.ofNat_ne_zero,
      not_false_eq_true,
      zero_pow,
      mul_zero
    ]


in fact the minimum is x = h

-----------------

Can you tell me more about the equivalence between - (Fin n → ℝ) →L[ℝ] ℝ and Fin n → ℝ.



Yes — this is a really nice piece of linear algebra that mathlib formalises.

The spaces involved
- Fin n → ℝ is the standard n-dimensional real vector space, i.e. \mathbb{R^{\mathnormal{n}}}.
- (Fin n → ℝ) →L[ℝ] ℝ is the space of continuous linear maps from \mathbb{R^{\mathnormal{n}}} to \mathbb{R}. In finite dimensions, every linear map is continuous, so this is just the dual space (\mathbb{R^{\mathnormal{n}}})^*.

The equivalence
There is a canonical linear isometry equivalence
(\mathbb{R^{\mathnormal{n}}})^*\; \; \cong \; \; \mathbb{R^{\mathnormal{n}}}
given by the Riesz representation theorem in finite-dimensional Euclidean spaces:
- For each functional f:\mathbb{R^{\mathnormal{n}}}\rightarrow \mathbb{R}, there exists a unique vector v\in \mathbb{R^{\mathnormal{n}}} such that
f(w)=\langle v,w\rangle - for all w\in \mathbb{R^{\mathnormal{n}}}.
- The correspondence f\mapsto v is linear and norm-preserving (an isometry).
In Lean/mathlib this is packaged as a LinearIsometryEquiv called something like:ContinuousLinearMap.toEuclideanDual (Fin n → ℝ)
This gives you an equivalence((Fin n → ℝ) →L[ℝ] ℝ) ≃ₗᵢ[ℝ] (Fin n → ℝ)
How it works concretely- If you take a functional x : (Fin n → ℝ) →L[ℝ] ℝ, you can recover its representing vector by evaluating it on the standard basis vectors:
v i := x (Pi.single i 1)
- This is exactly the coordinate vector of x.
- Conversely, given a vector v : Fin n → ℝ, you can build the functional
f w := ∑ i, v i * w i
- which is linear and continuous.
These two constructions are inverses, and they preserve norms.Why it mattersThis equivalence is what lets you transport the inner product structure from Fin n → ℝ onto its dual (Fin n → ℝ) →L[ℝ] ℝ. Once you do that, all the axioms (norm_sq_eq_re_inner, conj_inner_symm, etc.) become immediate consequences of the corresponding facts in Fin n → ℝ.So in short:- (Fin n → ℝ) →L[ℝ] ℝ is the dual space.
- Fin n → ℝ is the primal space.
- The Riesz representation theorem gives a canonical linear isometry equivalence between them, implemented in mathlib as ContinuousLinearMap.toEuclideanDual.
Would you like me to show you the exact Lean definition of ContinuousLinearMap.toEuclideanDual so you can see how it’s constructed?

-----------------




